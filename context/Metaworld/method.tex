\newcommand{\1}{\mathbb{I}}

\section{Meta-World}
\vspace{-0.2cm}

%%CF: Would love for someone to take a pass on the below and give suggestions / make edits. Not sure if it's too formulaic or verbose. Though, I would also like to get across that we put careful consideration of many design decisions into the benchmark.

If we want meta-RL methods to generalize effectively to entirely new tasks, we must meta-train on broad task distributions that are representative of the range of tasks that a particular agent might need to solve in the future. %Such a suite of diverse tasks would exercise the full potential of both meta-RL and multi-task RL methods, and must consist of a large number of tasks spanning both parametric and non-parametric degrees of variation.
To this end, we propose a new multi-task and meta-RL benchmark, which we call Meta-World. In this section, we motivate the design decisions behind the Meta-World tasks, discuss
%discuss %the design decisions behind Meta-World,
the range of tasks, describe the representation of the actions, observations, and rewards, and present a set of evaluation protocols of varying difficulty for both meta-RL and multi-task RL.

%\subsection{Design Considerations for a Multi-Task and Meta-RL Benchmark}

%We now describe the Meta-World benchmark, with which we aim to provide both a widely accessible suite of tasks and a standard evaluation protocol for meta-learning and multi-task learning RL methods.
%An effective benchmark must (1) reflect important, realistic, and unsolved problems, such that progress on the benchmark leads to meaningful progress on real problems; (2) be at the right level of difficulty: challenging for existing methods, yet not too difficult as to make research progress impossible; (3) be widely accessible and easy to use.
%We design MetaWorld with each of these considerations in mind.
%%SL: commented out above sentence since it should be obvious (just trying to cut down on space a bit)
%The primary unsolved problem that we focus on for is the meta-RL problem: learning models that can quickly acquire new tasks by leveraging experience from other, related prior tasks. Since this problem is unsolved, we cannot ensure (2) fully. However, we can provide versions of our benchmark at variable levels of difficulty, such that easier variants demand less broad generalization. By ensuring that easier versions of our benchmark can be solved with current methods, we provide a tiered evaluation protocol that will enable progress of meta-RL and multi-task RL methods.

%For (1), the particular real problem we focus on primarily is the meta-RL problem of quickly learning distinctly new tasks by leveraging experience on other, previously seen tasks. Since this problem is inherently unsolved, it is impossible to ensure (2). However, there are a number of steps that will make the benchmark well-suited for future research. This includes: making versions of the evaluation at variable levels of difficulty, ensuring that the easiest versions of the benchmark can be solved by current methods and, most importantly, taking careful consideration of the distribution of tasks to ensure both diversity and shared structure.
%%Without both of these, generalization to new tasks is not possible.
%%SL: likewise I think that last sentence can be cut? but feel free to uncomment if you think it's critical

%%SL.7.6: moved this to the top
%In the remainder of this section, we will discuss each of these challenges and design decisions in greater detail, share the insights we learned when designing task distributions that are well suited for algorithms to make progress on, and describe the evaluation protocol, both for meta-learning algorithms as well as for multi-task learning methods.


%Since generalization to new tasks requires a sufficient diversity of meta-training tasks, MetaWorld consists of $50$ qualitatively distinct manipulation task families.
%%SL.7.4: can we reference a figure here?
%Each of these task families itself consists of an infinite number of tasks, which can be generated by varying the start and goal positions of objects in the scene.
%For each task, different variations such as goal positions are also implemented, which are used to evaluate if meta-RL algorithms can generalize to new variation within a control task, and also to facilitate the potential for greater overlap across distinct tasks.
%Below, we discuss the design of the tasks, the parametric variability in each task family, the reward functions and binary success metrics for evaluation, and the proposed evaluation protocols.













%MetaWorld allows us to test if multi-task RL approaches can learn a single policy that can solve multiple tasks, and whether meta-RL algorithms can generalize to new tasks that were not seen during meta-training.

\subsection{The Space of Manipulation Tasks: Parametric and Non-Parametric Variability}
\label{sec:parametric}

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\columnwidth]{figures_v2/variation_cartoon.pdf}
    \caption{\footnotesize Parametric/non-parametric variation: all ``reach puck'' tasks (left) can be parameterized by the puck position, while the difference between ``reach puck'' and ``open window'' (right) is non-parametric.}
    \vspace{-0.5cm}
    \label{fig:variation-catoon}
\end{wrapfigure}

%\cf{This section should motivate our design choices about talking about the importance of parametric and non-parametric variation, without being too repetitive with the problem statements above. Hence, if we hope to scale these algorithms to more realistic domains, such as the domain of table top manipulation tasks, we requires a suite of tasks that reflects the breadth of tasks in the domain while structured in a way that is still amenable to transfer (albeit a more difficult form of transfer than previous evaluations). However, with enough tasks, even two qualitatively different tasks may overlap, serving as a catalyst for generalization. For example, consider a robot that needs to push a block away from itself versus push a drawer closed. For some initial positions of the object and the drawer, these tasks are the nearly the same, \emph{as long as the reward functions is structured in a similar way}.}

A task, $\task$, in Meta-World is defined as the tuple \textit{(reward\; function, \;initial \;object \;position, \;target \;position)}
Meta-learning makes two critical assumptions: first, that the meta-training and meta-test tasks are drawn from the same distribution, $p(\task)$, and second, that the task distribution $p(\task)$ exhibits shared structure that can be utilized for efficient adaptation to new tasks. 
If $p(\task)$ is defined as a family of variations within a particular control task, as in prior work~\cite{finn2017model,rakelly2019efficient}, then it is unreasonable to hope for generalization to entirely new control tasks. For example, an agent has little hope of being able to quickly learn to open a door, without having ever experienced doors before, if it has only been trained on a set of meta-training tasks that are homogeneous and narrow. 
Thus, to enable meta-RL methods to adapt to entirely new tasks, we propose a much larger suite of tasks consisting of $50$ qualitatively-distinct manipulation tasks, where continuous parameter variation cannot be used to describe the differences between tasks.

%Thus, instead, the tasks in the task distribution must be

%structurally similar for transfer to be successful, yet sufficiently diverse to achieve generalization to distinctly new tasks.

%Therefore, learning such structure is regarded as the main goal of meta-learning. In prior meta-RL work, $p(\task)$ is defined as a family of variations within a particular control task~\cite{finn2017model, rakelly2019efficient}, such as a distribution of goal velocities for a running robot. How can we hope to generalize to entirely new tasks -- e.g., generalize such that the agent can quickly learn to open a door without having ever experienced doors before -- when the set of meta-training tasks is so homogeneous and narrow?




% Each of the manipulation tasks in our benchmark defines a full task family, where individual tasks can be obtained by instantiating the task-relevant objects (e.g., the door in the door opening task) at a randomized location and orientation, and with a randomized goal. The range of potential start and goal positions and orientations for each object is defined for each task. 
%If the locations are not fixed, this kind of memorization is impossible, and the model is forced to generalize more broadly. Note that this kind of parametric variation, which we introduce \emph{for each task family}, essentially represents the entirety of the task distribution for previous meta-RL evaluations~\cite{finn2017model, rakelly2019efficient}, which test on single task families (e.g., running in a direction) with parametric variability (e.g., variation in the goal direction). Our full task distribution is therefore substantially broader, since it includes this parametric variability \emph{for each of the $50$ task families}. In order to evaluate multi-task RL methods, we also provide a parametrically static version of the tasks, where the object and goal configurations are fixed within each task.


With such non-parametric variation, however, there is the danger that tasks will not exhibit enough shared structure, or will lack the task overlap needed for the method to avoid memorizing each of the tasks. 
Motivated by this challenge, we design each task to include parametric variation in object and goal positions, as illustrated in Figure~\ref{fig:variation-catoon}.
Introducing this parametric variability not only creates a substantially larger (infinite) variety of tasks, but also makes it substantially more practical to expect that a meta-trained model will generalize to acquire entirely new tasks more quickly, since varying the positions provides for wider coverage of the space of possible manipulation tasks. Without parametric variation, the model could for example memorize that any object at a particular location is a door, while any object at another location is a drawer. 
If the locations are not fixed, this kind of memorization is much less likely, and the model is forced to generalize more broadly. 
With enough tasks and variation within tasks, pairs of qualitatively-distinct tasks are more likely to overlap, serving as a catalyst for generalization.
For example, closing a drawer and pushing a block can appear as nearly the same task for some initial and goal positions of each object.


Note that this kind of parametric variation, which we introduce \emph{for each task}, essentially represents the entirety of the task distribution for previous meta-RL evaluations~\cite{finn2017model, rakelly2019efficient}, which test on single tasks (e.g., running towards a goal) with parametric variability (e.g., variation in the goal position). Our full task distribution is therefore substantially broader, since it includes this parametric variability \emph{for each of the $50$ tasks}.


To provide shared structure, the $50$ environments require the same robotic arm to interact with different objects, with different shapes, joints, and connectivity. The tasks themselves require the robot to execute a combination of reaching, pushing, and grasping, depending on the task.
By recombining these basic behavioral building blocks with a variety of objects with different shapes and articulation properties, we can create a wide range of manipulation tasks. For example, the \textbf{open door} task
%%SL.7.4: reference a figure
involves pushing or grasping an object with a revolute joint, while the \textbf{open drawer} task requires pushing or grasping an object with a sliding joint.
%%SL.7.4: reference a figure
More complex tasks require a combination of these building blocks, which must be executed in the right order.
We visualize all of the tasks in Meta-World in Figure~\ref{fig:ml45_teaser}, and include a description of all tasks in Appendix~\ref{app:tasks}.
%We list some examples of these more complex tasks below, provide visualizations of all the tasks in Figure~\ref{fig:teaser}, and include a description of all tasks in Appendix~\ref{app:tasks}.

%%SL.7.6: My suggestion would be to cut the examples below -- I think a picture is worth a thousand words here, and the reader will get a much clearer idea of what we are actually doing by looking at Figure 1 (which should be made, and should be good). We're going to run out of space, so in terms of thinking about what we can cut, I feel like these three examples are the most cuttable
\iffalse
\textbf{Button pressing:} The robot needs to press a button located on the wall. This task is a composition of reaching and pushing primitives, as the robot is required to first reach the button and then push it all the way to the end.
%%SL.7.4: I'm not sure this is the right task to highlight -- we mentioned before that we are going to talk about more complex tasks, but this task seems to be quite trivial. Also, reference a figure.
\todo{Kevin / Deirdre -- please pick a different example for this one.}

\textbf{Shelf placing:} The robot needs to place an object onto a 2-level shelf. Reaching the object, grasping the object, and placing the object on the shelf should be performed sequentially for this task.
%%SL.7.4: Reference a figure.

\textbf{Stick pushing:} The robot needs to pick up a stick and use it to push a box to a goal position. In this task, the sequence of primitives to be performed is reaching the stick, grasping the stick, placing the stick near the box, and pushing the box.
%%SL.7.4: Reference a figure.
\fi

All of the tasks are implemented in the MuJoCo physics engine~\cite{todorov2012mujoco}, which enables fast simulation of physical contact. 
To make the interface simple and accessible, we base our suite on the Multiworld interface~\cite{nair2018visual} and the OpenAI Gym environment interfaces~\cite{brockman2016openai}, making additions and adaptations of the suite relatively easy for researchers already familiar with Gym.


\iffalse
%%CF.7.5: I'm commenting this out, since it is covered in the previous section now.
\subsection{Parametric Variability}
\label{sec:parametric}

To encourage easier generalization to new tasks,
%%SL.7.4: Let's try to stay away from the word "encourage" -- it's very informal and imprecise, and doesn't really sound very flattering in an optimization context.
in addition to designing a large number of heterogeneous tasks, having a heavily overlapped observation space among all tasks is conducive.
%%SL.7.4: This seems like a pretty roundabout way of putting it. Maybe say something like: In order to be able to train one model on all of the tasks, we must define a state representation that is compatible with all of the task domains.
The rationale is that the agent is more inclined to discover the underlying shared structure among tasks if it interacts with a variety of environments that enjoy similar but varied representations. We enable such representations of the observations by drawing positions of objects and goals randomly from a common set across tasks and setting them parametrically.
%%SL.7.4: I'm confused, I thought you were talking about representations (i.e., how the state vector is put together), but now suddenly this is talking about positions of objects and goals? Overall, I don't think this paragraph really makes much sense the way it is written. Maybe a potential rephrasing could go like this:
% Each of the manipulation tasks in our benchmark defines a full task family, where individual tasks can be obtained by instantiating the task-relevant objects (e.g., the door in the door opening task) at a randomized location and orientation, and with a randomized goal. The range of potential start and goal positions and orientations for each object is defined for each task. Introducing this parametric variability not only creates a substantially larger (infinite) variety of tasks, but also makes it substantially more practical to expect that a meta-trained model will generalize to acquire entirely new tasks more quickly, since varying the positions provides for wider coverage of the space of possible manipulation tasks. Without parametric variation, the model could for example memorize that any object at a particular location is a door, while any object at another location is a drawer. If the locations are not fixed, this kind of memorization is impossible, and the model is forced to generalize more broadly. Note that this kind of parametric variation, which we introduce \emph{for each task family}, essentially represents the entirety of the task distribution for previous meta-RL evaluations~\cite{finn2017model, rakelly2019efficient}, which test on single task families (e.g., running in a direction) with parametric variability (e.g., variation in the goal direction). Our full task distribution is therefore substantially broader, since it includes this parametric variability \emph{for each of the $50$ task families}. In order to evaluate multi-task RL methods, we also provide a parametrically static version of the tasks, where the object and goal configurations are fixed within each task.
Therefore, the robot is expected to experience similar but a wide range of object and goal configurations across all tasks.

Note that the distribution of such a parametric variability is essentially the task distribution used for evaluating previous meta-RL algorithms ~\cite{finn2017model, rakelly2019efficient}. Hence our task distribution strictly covers the distributions used in prior work and poses a more challenging adaptation problem for meta-RL algorithms.

In order to evaluate multi-task RL methods, we also provide a parametrically static version of the tasks, where the object and goal configurations are fixed within each task.
\fi

\subsection{Actions, Observations, and Rewards}



%\todo{Need to describe the observation space here, whether or not it includes goal information, how we make it fixed in length, etc. Can motivate using low-dim state space by saying that we don't want it to be too hard (yet), similar to the motivation in the next paragraph}

In order to represent policies for multiple tasks with one model, the observation and action spaces must contain significant shared structure across tasks. All of our tasks are performed by a simulated Sawyer robot. The action space is a 2-tuple consisting of the change in 3D space of the end-effector followed by a normalized torque that the gripper fingers should apply. The actions in this space range between $-1$ and $1$.
For all tasks, the robot must either manipulate one object with a variable goal position, or manipulate two objects with a fixed goal position. The observation space is represented as a 6-tuple of the 3D Cartesian positions of the end-effector, a normalized measurement of how open the gripper is, the 3D position of the first object, the quaternion of the first object, the 3D position of the second object, the quaternion of the second object, all of the previous measurements in the environment, and finally the 3D position of the goal. If there is no second object or the goal is not meant to be included in the observation, then the quantities corresponding to them are zeroed out. The observation space is always $39$ dimensional.
%A more general observation space would be to use image pixels; we leave this for future development, however, because RL with raw images presents a major challenge for existing RL methods, even without multiple tasks.

%While meta-RL algorithms can learn from this plain observation space, multi-task RL algorithms need to use a observation space that is augmented with task identities to retrieve task context information. We provide this augmented observation space by concatenating a one-hot vector, which represents a task identity, with the original observation.
%%CF: I commented out the above, since we now discuss it in section 5.

%\cf{This section should motivate by discussing the importance of having shared structure across the reward functions, along with designing tasks that are solveable.}

Designing reward functions for Meta-World requires two major considerations. First, to guarantee that our tasks are within the reach of current single-task reinforcement learning algorithms, which is a prerequisite for evaluating multi-task and meta-RL algorithms, 
%%CF.7.5: Arguably it's not a pre-requisite, since the other tasks should make learning easier. Can we rephrase? (still emphasizing that making the tasks easy enough is important)
we design well-shaped reward functions for each task that make each of the tasks at least individually solvable.
%%CF.7.5: Also mention that sparse reward settings is a straightforward extension of the benchmark.
More importantly, the reward functions must exhibit shared structure across tasks. Critically, even if the reward function admits the same optimal policy for multiple tasks, varying reward scales or structures can make the tasks appear completely distinct for the learning algorithm, masking their shared structure and leading to preferences for tasks with high-magnitude rewards~\cite{DBLP:journals/corr/abs-1809-04474}. 
%should also follow similar structure and have the same magnitudes across all tasks. Otherwise, meta- and multi-task- RL algorithms would favor the tasks that provider higher-magnitude rewards, while ignoring other tasks with lower reward magnitude.
Accordingly, we adopt a structured, multi-component
reward function for all tasks, which leads to effective policy learning for each of the task components. For instance, in a task that involves a combination of reaching, grasping, and placing an object, let $o \in \mathbb{R}^3$ be the object position, where $o = (o_x, o_y, o_z)$, $h \in \mathbb{R}^3$ be the position of the robot's gripper, $z_\text{target} \in \mathbb{R}$ be the target height of lifting the object, and $g \in \mathbb{R}^3$ be goal position. With the above definition, the multi-component reward function $R$ is the combination of a reaching reward, a grasping reward, and a placing reward or subsets thereof for simpler tasks that only involve reaching and/or pushing.
With this design, the reward functions across all tasks have a similar magnitude that ranges between $0$ and $10$, where $10$ always corresponds to the reward-function being solved, and conform to similar structure, as desired. The full form of the reward function and a list of all task rewards is provided in Appendix~\ref{app:rewardfns}.



\iffalse
\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/evaluation_table.png}
    \vspace{-0.6cm}
    \caption{List of environments used for evaluating multi-task and meta-RL algorithms.
    \chelsea{See comments.}
    }
    \vspace{-0.4cm}
    \label{fig:evaluation}
\end{figure}
\fi
%%SL.7.4: I'm not sure this is the best way to present the benchmarks. Maybe we can provide a full table listing all the tasks, a bit of information for each one, and for each one an indication of whether it is training/test and for which variant of the benchmark? Also, make sure to have an informative and clear caption.
%%CF: I removed this, since we can illustrate train/test splits for hard mode in Figure 1. I also described the train/test splits for the remaining modes in the text. I think it could still be good to have a very abbreviated version of this table that illustrates the protocols, but does not list out the tasks. We should also be sure to have a description of each of the tasks in the appendix.

\iffalse
\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c|c|c}
\hline
\footnotesize Task & Description & Roles \\
\hline
reach & reach position & MT, 1, 2tr, 3tr\\
push & & MT, 1, 2tr, 3tr\\
pick and place & & MT, 1, 2tr, 3tr\\
door open & & MT, 2tr, 3tr\\
\hline
\end{tabular}
\quad
\begin{tabular}{c|c|c}
\hline
Task & Description & Roles \\
\hline
bin pick &  &  3ts\\
box open &  &  3ts\\
box close &  &  3ts\\
door lock &  &  3ts\\
door unlock &  &  3ts\\
\hline
\end{tabular}
    \caption{CF: Here's a starter for what an alternative table might look like}
    \label{tab:my_label}
\end{table}
\fi

%%SL.6.29: A few high-level comments here:
% 3. It's important to motivate *why* we chose to do it this way, rather than throw out the protocol without any justification.
% 4. It's also important to be concrete and specific, maybe we add a table for each mode to list the particular tasks in each one?
\subsection{Evaluation Protocol}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures_v2/figure_3_metaworld.pdf}
    \caption{\footnotesize Visualization of three of our multi-task and meta-learning evaluation protocols, ranging from within task adaptation in ML1, to multi-task training across 10 distinct task families in MT10, to adapting to new tasks in ML10. Our most challenging evaluation mode ML45 is shown in Figure~\ref{fig:ml45_teaser}.
    }
    \label{fig:evaluation}
\end{figure}
With the goal of providing a challenging benchmark to facilitate progress in multi-task RL and meta-RL, we design an evaluation protocol with varying levels of difficulty, ranging from the level of current goal-centric meta-RL benchmarks to a setting where methods must learn distinctly new, challenging manipulation tasks based on diverse experience across 45 tasks. We hence divide our evaluation into five categories, which we describe next. We then detail our evaluation criteria.


%Tasks used for each category are presented in Figure~\ref{fig:evaluation}.

%%SL.7.4: While I appreciate not having "baby" in there, perhaps it would be best for us to name these evaluations by some concise one or two word name that can be used to refer to them, that is somewhat more informative than just a number? E.g., "single-family, small-scale, large-scale?"
\textbf{Meta-Learning 1 (ML1): Few-shot adaptation to goal variation within one task.} The simplest evaluation aims to verify that previous meta-RL algorithms can adapt to new object or goal configurations on only one type of task. ML1 uses single Meta-World Tasks, with the meta-training ``tasks'' corresponding to $50$ random initial object and goal positions, and meta-testing on $50$ held-out positions. This resembles the evaluations in prior works~\cite{finn2017model, rakelly2019efficient}. We evaluate algorithms on three individual tasks from Meta-World: reaching, pushing, and pick and place, where the variation is over reaching position or goal object position. The goal positions are not provided in the observation, forcing meta-RL algorithms to adapt to the goal through trial-and-error.

\textbf{Multi-Task 1 (MT1): Learning one multi-task policy that generalizes to 50 tasks belonging to the same environment}. This evaluation aims to verify how well multi-task algorithms can learn across a large related task distribution. MT1 uses single Meta-World environments, with the training ``tasks'' corresponding to $50$ random initial object and goal positions. The goal positions are provided in the observation and are a fixed set, as to focus on the ability of algorithms in acquiring a distinct skill across multiple goals, rather than generalization and robustness.

\textbf{Multi-Task 10, Multi-Task 50 (MT10, MT50): Learning one multi-task policy that generalizes to 50 tasks belonging to 10 and 50 training environments, for a total of 500, and 2,500 training tasks.} A first step towards adapting quickly to distinctly new tasks is the ability to train a single policy that can solve multiple distinct training tasks. The multi-task evaluation in Meta-World tests the ability to learn multiple tasks at once, without accounting for generalization to new tasks. The MT10 evaluation uses 10 environments: reach, push, pick and place, open door, open drawer, close drawer, press button top-down, insert peg side, open window, and open box. The larger MT50 evaluation uses all $50$ Meta-World environments. In our experiments, the algorithm is typically provided with a one-hot vector indicating the current task. The positions of objects and goal positions are fixed in all tasks in this evaluation, so as to focus on acquiring the distinct skills, rather than generalization and robustness.

%A first step towards adapting quickly to distinctly new tasks is the ability to train a single policy that can solve multiple distinct training tasks.
%We develop an multi-task evaluation, MT10, on 10 tasks: reach, push, pick and place, open door, open drawer, close drawer, pess button top-down, insert peg side, open window, and open box, as well as en evaluation on all $50$ MetaWorld tasks (MT50).
%listed in Figure~\ref{fig:evaluation}.
%%SL.6.29: which ones?
% Since there do not need to evaluate generalization to new tasks,
%%SL.6.29: This is phrased in contrast to the meta-learning section below, which does require generalization, but it comes *before* the meta-learning section, so the reader won't understand the significance of this.
%Since multi-task RL algorithms require task identifications as input to the policy, w
%We identify each task with a one-hot encoding, which is provided with the current state information to multi-task policy. As we are evaluating the ability to acquire 10 qualitatively different tasks, we fix the position of the objects and goal positions in each task to ease multi-task RL training and encourage the RL agent to focus on learning different skills. 


\textbf{Meta-Learning 10, Meta-Learning 45 (ML10, ML45): Few-shot adaptation to new test tasks with 10 and 50 meta-training tasks.}  With the objective to test generalization to new tasks, we hold out 5 tasks and meta-train policies on 10 and 45 tasks.
We randomize object and goals positions %, as per Section~\ref{sec:parametric}, 
and intentionally select training tasks with structural similarity to the test tasks. Task IDs are not provided as input, requiring a meta-RL algorithm to identify the tasks from experience.
%Similar to ML9, object and goal positions are varied and task IDs are not provided in the observation space.

%%SL.7.4: Overall, I'm pretty concerned that the current description of the meta-learning protocol is very hard to understand. Maybe let me know once you've had a chance to revise, and I'll take another quick look (and/or edit it directly)?
%%CF: I took another pass.

\textbf{Success metrics.} 
Since values of reward are not directly indicative how successful a policy is, we define an interpretable success metric for each task, which will be used as the evaluation criterion for all of the above evaluation settings.
Since all of our tasks involve manipulating one or more objects into a goal configuration, this success metric is typically based on the distance between the task-relevant object and its final goal pose, i.e. $\|o - g\|_2 < \epsilon$, where $\epsilon$ is a small distance threshold such as $5$ cm. For the complete list of success metrics and thresholds for each task, see Appendix~\ref{tbl:task_metrics}.
%%SL.7.4: Is it 5 cm everywhere? If not, reference an appendix and give a table of thresholds in the appendix. Being precise is important when describing an open-source benchmark.





%\subsection{Multi-Task Evaluation Protocol}
%%SL.7.4: I would put this last (or just delete it -- at this point I'm really starting to get the impression that keeping the multi-task stuff around harms more than it helps).

%%KH.7.2 Can we try to keep the equal focus on both, multi-task RL and meta-RL? Maybe we can avoid saying that the main goal is to study meta-learning. We should also write here the evaluation protocol for multi-task RL (preferably written in a similar form as meta-RL).
%%KH.7.2 Should we also evaluate multi-task RL in the hard mode?

