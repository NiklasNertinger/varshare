\section{Related Work}

Algorithms for multi-task learning typically consider how to train a single model that can solve a variety of different tasks~\citep{caruana97multitask, bakker2003clustering, ruder2017overview}.
The multi-task formulation has been applied to many different settings, including supervised learning~\citep{zhang2014facial, long2015learning, yang2016trace, sener2018multi, zamir2018taskonomy} and reinforcement-learning~\citep{espeholt2018impala, wilson2007multi}, as well as many different domains, such as vision~\citep{bilen2016integrated, misra2016cross, kokkinos2017ubernet, liu2018attention, zamir2018taskonomy}, language~\citep{collobert2008unified, dong2015multi, mccann2018natural, radford2019language} and robotics~\citep{riedmiller2018learning, wulfmeier2019regularized, hausman2018learning}.
While multi-task learning has the promise of accelerating acquisition of large task repertoires, in practice it presents a challenging optimization problem, which has been tackled in several ways in prior work.

A number of architectural solutions have been proposed to the multi-task learning problem based on multiple modules or paths~\citep{fernando2017pathnet, devin2016modularnet, misra2016crossstitch, rusu2016progressive, rosenbaum2017routing, vandenhende2019branched, rosenbaum2017routing}, or using attention-based architectures~\citep{liu2018attention, maninis2019attention}. Our work is agnostic to the model architecture and can be combined with prior architectural approaches in a complementary fashion.
A different set of multi-task learning approaches aim to decompose the problem into multiple local problems, often corresponding to each task, that are significantly easier to learn, akin to divide and conquer algorithms~\citep{levine2016end,rusu2015policydistillation,parisotto2015actor,teh2017distral, ghosh2017dnc, czarneki2019distilling}. 
Eventually, the local models are combined into a single, multi-task policy using different distillation techniques (outlined in~\citep{hinton2015distilling,czarneki2019distilling}).
In contrast to these methods, we propose a simple and cogent scheme for multi-task learning that allows us to learn the tasks simultaneously using a single, shared model without the need for network distillation.


Similarly to our work, a number of prior approaches have observed the difficulty of optimization in multi-task learning~\citep{hessel2019popart, kendall2018multitask, schaul2019ray, suteu2019regularizing}. Our work suggests that the challenge in multi-task learning may be attributed to what we describe as the tragic triad of multi-task learning (i.e., conflicting gradients, high curvature, and large gradient differences), 
which we address directly by introducing a simple and practical algorithm that deconflicts gradients from different tasks. Prior works combat optimization challenges by rescaling task gradients ~\cite{sener2018multi,chen2018gradnorm}. We alter both the magnitude and direction of the gradient, which we find to be critical for good performance (see Fig.~\ref{fig:rl_results}). 
Prior work has also used the cosine similarity between gradients to define when an auxiliary task might be useful~\citep{du2018aux} or when two tasks are related~\cite{suteu2019regularizing}. We similarly use cosine similarity between gradients to determine if the gradients between a pair of tasks are in conflict. %
Unlike~\citet{du2018aux}, we use this measure for effective multi-task learning, instead of ignoring auxiliary objectives.
Overall, we empirically compare our approach to a number of these prior approaches~\cite{sener2018multi,chen2018gradnorm,suteu2019regularizing}, and observe superior performance with PCGrad.
%

\arxiv{Multiple approaches to continual learning have studied how to prevent gradient updates from adversely affecting previously-learned tasks through various forms of gradient projection\icmllast{~\citep{lopez2017gradient, agem, farajtabar2019orthogonal, guo2020improved}}. These methods focus on sequential learning settings, and solve for the gradient projections using quadratic programming~\citep{lopez2017gradient}, only project onto the normal plane of the average gradient of past tasks~\citep{agem}, or project the current task gradients onto the orthonormal set of previous task gradients~\citep{farajtabar2019orthogonal}. In contrast, our work focuses on positive transfer when simultaneously learning multiple tasks, does not require solving a QP, and \emph{iteratively} projects the gradients of each task instead of \emph{averaging} \icmllast{or only projecting the \emph{current} task gradient.}} Finally, our method is distinct from and solves a different problem than the projected gradient method~\citep{gradient_projection}, which is an approach for constrained optimization that projects gradients onto the constraint manifold. 
