\label{related}
\textbf{Multi-task learning.} Multi-task learning~\cite{caruana1997multitask} is one of the core machine learning problems. Researchers have shown learning with multiple objectives can make different tasks benefit from each other in robotics and RL ~\cite{wilson2007multi,pinto2016curious,pinto2017learning,riedmiller2018learning,hausman2018learning,sax2019learning}. While sharing parameters across tasks can intuitively improve data efficiency, gradients from different tasks can interfere negatively with each other.
% This phenomenon is more severe in the context of RL.
One way to avoid it is to use policy distillation~\cite{parisotto2015actor,rusu2015policy,teh2017distral, teh2017distral}.
% For example, ~\cite{teh2017distral} propose to share a distilled policy capturing common properties across tasks, and then train different task-specific policies by constraining them close to the shared policy.
However, these approaches still require separate networks for different policies and an extra distillation stage. 
% To handle the optimization problem in multi-task learning,
Researchers also propose to explicitly model the similarity between gradients from different tasks~\cite{zhang2014regularization,chen2017gradnorm,kendall2018multi,lin2019adaptive,sener2018multi,du2018adapting,Yu2020surgery,hu2019learning}. For example, it is proposed in~\cite{chen2017gradnorm} to normalize the gradients from different tasks for balancing multi-task losses. Besides adjusting the losses, a recent work~\cite{Yu2020surgery} proposes to directly reduce the gradient conflicts by gradient projection. However, optimization relying on the gradient similarity is usually unstable, especially when there is a large gradient variance within each task itself. 

% ~\cite{du2018adapting} propose to compute the similarity between gradients from the auxiliary task and the main task and use it to adapt auxiliary tasks. 

\textbf{Compositional learning and modularization.} Instead of directly enforcing the gradients to align, a natural way to avoid the conflicts of the gradient is using compositional models. By utilizing different modules across different tasks, it reduces the interference of gradients on each module and allows better generalization~\cite{singh1992transfer,devin2017learning,andreas2017modular,rusu2016progressive,yuzhe2020,peng2019mcp,haarnoja2018composable,sahni2017learning,Goyal2020Reinforcement}. For example, the policy is decomposed to task-specific and robot-specific modules in~\cite{devin2017learning}, and the policy is able to solve unseen tasks by re-combining the pre-defined modules. However, the pre-defining modules and manual specification of the combination are not scalable. Instead of defining and pre-training the modules or sub-policies, %our approach seeks to learn a routing network to estimate a soft combination of the modules for each task, and the modules will be learned at the same time. 
our approach utilizes soft combinations over modules, which allows fully end-to-end training. %%joint training in an end-to-end fashion. 


There are also related works learning a routing module in the supervised tasks.  Rosenbaum et. al~\cite{rosenbaum2017routing,rosenbaum2019routing} proposes to use RL to learn a routing policy, which can be considered as a \emph{hard} version of our soft modularization method. This ``hard modularization'' approach is infeasible for RL settings because joint training a multitask control policy and a routing policy suffers from exponentially higher variance in policy gradient due to the temporal nature in RL and leads to severe training instability. For RL, ``hard modularization'' approach would further introduce high variance along with the exploration in the environment. Whereas, our soft version doesn't introduce additional variance, significantly stabilizes RL training, and produces much improved empirical performances. In addition, the following works inspire our work in different ways:  Purushwalkam et. al \cite{purushwalkam2019task} consider zero-shot compositional learning in vision; Wang et.al \cite{wang2019tafe} consider weight generation for multi-task learning; Li et.al \cite{li2020learning} alleviate the scale variance in semantic representation with dynamic routing.

\textbf{Mixture of experts.} Our method is also related to works on mixture of experts~\cite{MI1993,gomi1993,jacobs1991,singh1992transfer,NIPS1993_750,Ma2018MMoE}. For example, it is proposed in Satinder et. al~\cite{singh1992transfer} to train a gating function to select different Q functions (experts) for different tasks. Instead of performing one-time selection among the individual expert (which is usually pre-defined), the modules we propose are organized in multiple layers in our base policy network, with multiple layers of selection guided by the routing network. While each module alone is not functioning as a policy, this increases the flexibility of sharing the modules across different tasks. At the same time, it reduces the mutual interference between the modules because the modules are only connected via the routing network. 

