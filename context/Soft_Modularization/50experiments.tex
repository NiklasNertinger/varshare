We perform experiments on multi-task robotics manipulation. We discuss the experiment environment, benchmark, and baselines,  compare our method with baselines and conduct ablation study. 

\vspace{-0.07in}
\subsection{Environment}
\vspace{-0.07in}
We evaluate our approach with the recent proposed Meta-World~\cite{yu2019meta} environment. This environment contains 50 different robotics continuous control and manipulation tasks with a sawyer arm in the MuJoCo environment~\cite{todorov2012mujoco}. There are two challenges for multi-task learning in this environment: MT10 and MT50 challenge, which requires learning 10 and 50 manipulation tasks simultaneously. Building on top of these two challenges, we further extend the tasks to be goal-conditioned tasks. More specifically, the original MT10 and MT50 tasks are manipulation tasks with fixed goals. To make the tasks more realistic, we extend the tasks to have flexible goals. We name the two extensions as \textbf{MT10-Conditioned} and \textbf{MT50-Conditioned} tasks meanwhile we denote the original MT10 challenge as \textbf{MT10-Fixed} and the original MT50 challenge as \textbf{MT50-Fixed}.
 
% \begin{minipage}

% \end{figure}
% \end{minipage}


\vspace{-0.07in}
\subsection{Baselines and Experimental Settings}
\vspace{-0.07in}

\textbf{Baselines.} We train our model with SAC~\cite{DBLP:journals/corr/abs-1801-01290}. We compare to five baselines with SAC without using our network architecture as following: (i) \textbf{Single-task SAC}: Individual policy for each task in MT10-Conditioned.
(ii) \textbf{Multi-task SAC (MT-SAC)}: Using a one-hot task ID with the state as inputs. (iii) \textbf{Multi-task multi-head SAC (MT-MH-SAC)}: Built upon MT-SAC with independent heads for tasks. The same MT-SAC and MT-MH-SAC baselines are also proposed in~\cite{yu2019meta}, and we reproduce their results. (iv) \textbf{Mixture of Experts (Mix-Expert)}: It consists of four experts with the same architecture as MT-SAC, and a learned gating network for expert combination~\cite{jacobs1991}. (v) \textbf{Hard Routing}: It consists of four module layers with four modules each layer. For each layer, agent selects one module to use according to the controller/router depending on the task, following~\cite{rosenbaum2017routing}.



\textbf{Variants of Our Approach.} We conduct all experiments under two settings of our method. We ablate different numbers of module layer and modules in each layer: \textbf{Ours (Shallow)} contains $L=2$ module layers, $n=2$ modules per layer and each module outputs a $d=256$ representation; \textbf{Ours (Deep)} contains $L=4$ module layers, $n=4$ modules per layer and each module outputs a $d=128$ representation. The number of parameters is the same in both cases.


\textbf{Evaluation Metrics.} We evaluate the policies based on the success rate of executing the tasks, which is well-defined in the Meta-World environment\cite{yu2019meta}. We use the average success rate cross tasks to measure the performance. For each experiment, we train all methods with 3 random seeds. To plot the training curves, we plot the success rate of the polices across time with variance. For the final performance, we directly evaluate the final policy for each approach. We sample 100 episodes per task per seed. We compute the success rate for all these trials and report the averaged results. 

\textbf{Training samples.} For MT-SAC and MT-MH-SAC baselines, we train them with 20 million samples on the MT10 setting and 100 million samples on the MT50 setting. For our method, Mix-Expert and Hard Routing Baselines, they converge much faster, and we train it with 15 million samples for MT10 and with 50 million samples for MT50 tasks. 


\vspace{-0.1in}
\subsection{Routing Network Visualization}
\vspace{-0.1in}

We perform visualization on the networks trained with Ours (Deep) on the MT10-Conditioned setting. 

\begin{figure}
\centering
    % \subfloat[]{
        \includegraphics[width=0.22\textwidth, clip]{figs/compare-drawer-close-v1-ped-insert-side-v1.pdf}
    % }
    % \subfloat[]{
        \includegraphics[width=0.22\textwidth, clip]{figs/compare-push-v1-window-open-v1.pdf}
    % }
    % \subfloat[]{
        \includegraphics[width=0.22\textwidth, clip]{figs/compare-reach-v1-pick-place-v1.pdf}
    % }
    % \subfloat[]{
        \includegraphics[width=0.22\textwidth, clip]{figs/compare-door-v1-drawer-open-v1.pdf}
    % }
    \vspace{-0.05in}
    \caption{\small{Sampled observation and corresponding routing. Each column shows two different tasks sharing similar routing. 
    % ((a) Close Drawer \& Insert Peg; (b) Push \& Close Window; (c) Reach \& Pick Place. (d) Open Door \& Open Drawer.).
    The shared parts are highlighted with blue boxes.}
    }
	\label{figure:demo}
    \vspace{-0.2in}
 \end{figure}


\begin{wrapfigure}{li}{0.42\textwidth}
\vspace{-0.2in}
\begin{center}
\includegraphics[width=0.42\columnwidth,clip]{figs/routing_visualization.png}
\vspace{-0.2in}
\caption{Probabilities from the routing network for different tasks are extracted and visualized with t-NSE. Routing probabilities from different tasks are grouped in different clusters.}
\label{figure:routing_tsne}
\vspace{-0.3in}
\end{center}
\end{wrapfigure}

\textbf{Probability Visualization.} We visualize the probabilities $p^l$ predicted by the routing network. Ours (Deep) contains $l=4$ module layers with $n=4$ modules per layer. As shown in Figure~\ref{figure:demo}, we plot $p^l$ as the connections between different modules and use deep red color to represent large probability and light red color for small probability. For each column, we visualize the routing networks for two different tasks. We can see that even for different tasks, they could share similar module connections. It shows that our soft modularization method allows the reuse of skills across different tasks. 

\textbf{t-SNE Visualization.} We visualize the routing probabilities for different tasks via t-SNE~\cite{Maaten08visualizingdata} in Figure~\ref{figure:routing_tsne}. We run the policy on each task in MT10-Conditioned multiple times to collect routing samples. We combine all the routing probabilities from all layers into a $(l-1)n^2=48$ dimensional vector representing the routing path and visualize via t-SNE. We find clear boundaries between tasks, indicating that the agent can distinguish different tasks and choose corresponding skillset for each task. Besides, we notice that those tasks sharing similar task structures (e.g., drawer-open-v1 and drawer-close-v1, window-open-v1 and window-close-v1) are close in the plot. 

\vspace{-0.1in}
\subsection{Quantitative Results}
\vspace{-0.1in}


\textbf{Results on MT10-Fixed.} As shown in Table~\ref{tab:MT50andMT10}, our re-implementation of multi-task multi-head SAC performs very close to the reported results in~\cite{yu2019meta}. Although the final success rate of our method is only $2\%$ better than our best baseline implementation, our method converges faster than the baselines, as shown in the 2nd plot in Figure~\ref{figure:general_comparison}. We are not getting a significant gain in the final success rate is because training 10 tasks with fixed goals is quite simple. We move forward to a more practical and challenging setting with training  goal-conditioned policies. 

\textbf{Results on MT10-Conditioned.} As task difficulty increases, we can see from Table~\ref{tab:MT50andMT10} that our approach (Ours (Shallow)) achieves more than $4\%$ improvement over the baseline. Our approaches continue to improve the sample efficiency over the MT-MH-SAC baselines (1st plot in Figure~\ref{figure:general_comparison}). 

\begin{figure}[t]
\begin{center}
    \centering
        \includegraphics[width=\textwidth, clip]{figs/all_training_curve_nips.pdf}
        \vspace{-0.25in}
        \caption{Training curves of different methods on all benchmarks (Concrete lines: the average over 3 seeds; Shaded areas: the standard deviation over 3 seeds). For MT10, our method converges much faster than the baselines. For MT50, we achieve a large gain on sample efficiency and performance.}
    \label{figure:general_comparison}
\end{center}
\vspace{-0.1in}
\end{figure}

\textbf{Results on MT50-Fixed and MT50-Conditioned.} When we are moving from joint training with 10 tasks to 50 tasks, the problem becomes more challenging. As shown in Table~\ref{tab:MT50andMT10} and the last two plots in Figure~\ref{figure:general_comparison}, our method achieves a significant improvement over the baseline methods (around $24\%$) in both the fixed goal and goal-conditioned settings. We also observe that in MT50 environments, Ours (Deep) performs better than Ours (Shallow) approach, while it is the opposite in the MT10 setting. The reason for this phenomenon might be: (i) for a smaller number of task (MT10), simple network topology facilitates more on information sharing across tasks; (ii) for larger number of task (MT50), more complex network topology provides more routing choices and prevents different tasks from harming the performance of each other. It is also worthy of mentioning that our method achieves better success rates in MT50-Conditioned environments than MT50-Fixed. The reason is that MT50-Conditioned provides more examples in training for better generalization.

\textbf{Mixture of Experts and Hard Routing baselines.} We notice that although the performance of Mixture of Experts is only close to MT-SAC on MT10, it performs better than MT-MH-SAC on MT50. The reason is that when the number of tasks is small, Mixture of Experts is easy to degenerate to MT-SAC (with a single network). When the number of tasks becomes larger, the gating network in mixture of experts can learn to cluster the tasks into different sets corresponding to different experts. Similarly, while the Hard Routing baseline performs poorly in MT10, it catches up with the MT-SAC when applied to 50 tasks. It shows that routing still helps when task number increases. However, the optimization with hard routing is extremely challenging (see discussions in Section~\ref{related}). Both baselines perform significantly worse than our method in both MT10 and MT50 tasks. 


\begin{table*}[t]
% \vspace{-0.1in}
\vspace{-0.05in}
\centering
\tablestyle{2pt}{1.05}
\begin{tabular}{l|x{50}x{80}x{50}x{80}}
\multicolumn{1}{c|}{Method} & MT10-Fixed & MT10-Conditioned & MT50-Fixed & MT50-Conditioned \\
\shline
MT-SAC$^{*}$        & 39.5\%          & -      & 28.8\%  & - \\
MT-SAC              & 44.0\%          & 42.6\% & 31.4\%  & 28.3\% \\
MT-MH-SAC$^{*}$     & \textbf{88.0\%} & -      & 35.9\%  & - \\
MT-MH-SAC           & 85.0\%          & 67.4   & 35.5\% & 34.2\% \\
Mix-Expert          & 42.8\%  & 40.0\% & 36.1\% & 37.5\% \\   
Hard Routing        & 20.8\%  & 27.0\% & 22.9\% & 29.1\% \\

\hline
Ours (Shallow) & 87.0\% & \textbf{71.8\%} & 59.5\% & 60.4\%  \\
Ours (Deep)    & 86.7\% & 68.4\% & \textbf{60.0\%} &  \textbf{61.0\%} \\
\end{tabular}
\vspace{-0.1in}
\caption{Comparisons on average success rates for MT10 and MT50 tasks. MT-SAC$^*$, MT-MH-SAC$^*$ indicate results reported in \cite{yu2019meta}. Approaches without $^*$ indicate baselines of our own implementation. \label{tab:MT50andMT10}}

\vspace{-0.2in}
\end{table*}


\vspace{-0.07in}
\subsection{Effects on Network Capacity}
\vspace{-0.07in}

We conduct experiments to see how the capacity of the network (number of parameters) can influence the performance of the baseline methods. We compare our approach with baselines using different numbers of parameters for MT50-Fixed in Table~\ref{tab:network_ablation}. We ablate different number of network layers and the number of hidden units in each layer. We denote MT-MH-SAC-$l$ as the multi-task multi-head SAC baseline with $l$ layers. We also ablate more hidden unites for each layer and name the methods with ``Wide''. The detailed configurations for different ablations are shown in Table~\ref{tab:network_ablation}. 

\begin{figure*}[t]
    \begin{center}
    \subfloat[MT50-Fixed]{
        \includegraphics[width=0.3\textwidth,clip]{figs/ablation_networksize_width_and_depth.png}
        \label{exp:capacity}
    }
    \subfloat[MT10-Conditioned]{
        \includegraphics[width=0.3\textwidth, clip]{figs/ablation.png}
        \label{exp:ablation1}
    }
    \subfloat[MT10-Conditioned]{
        \includegraphics[width=0.3\textwidth, clip]{figs/ablation_mtmhsac_balance.png}
        \label{exp:ablation2}
    }
    \vspace{-0.1in}
    \caption{\small{(a) Compare Ours (Deep) and baselines with different network capacity for MT50-Fixed. (b) Analyse balancing training samples and using observation for routing network in Ours (Shallow) for  MT10-Conditioned. (c) Analyse balancing training samples in the baseline for MT10-Conditioned.}}
	\label{figure:curve}
    \end{center}{}
\vspace{-0.1in}
\end{figure*}


\begin{table}
% {li}{0.6\textwidth}
% \vspace{-0.1in}
\tablestyle{2pt}{1.05}
\begin{tabular}{l|x{50}x{30}x{30}x{30}}
\multicolumn{1}{c|}{Method} & MT50-Fixed & Params & layers & units\\
\shline
MT-MH-SAC$^{*}$  & 35.9\% & 1.2x & 3 & 400\\
MT-MH-SAC        & 35.5\% & 1.2x & 3 & 400\\
MT-MH-SAC-4      & 46.7\% & 1.6x & 4 & 400\\
MT-MH-SAC-5      & 45.2\% & 2.0x & 5 & 400\\
MT-MH-SAC-6      & 45.0\% & 2.4x & 6 & 400\\

MT-MH-SAC-4-Wide$^{*}$     & 50.7\% & 3.3x & 4 & 600 \\
MT-MH-SAC-5-Wide$^{*}$     & 50.3\% & 4.2x & 5 & 600\\
\hline
Ours (Deep)    & \textbf{60.0\%} & 1x & - & - \\
\end{tabular}
\caption{Comparison with baselines using different number of parameters for MT50-Fixed.\label{tab:network_ablation}}
\vspace{-0.25in}
\end{table}


We observe that even our model uses the smallest number of parameters, we can still achieve much better results. For example, our method is around $10\%$ better than the baseline (MT-MH-SAC-5-Wide) which has $4.2$x number of parameters compared to our method. We also observe that the gain saturates very fast as we make the network larger and larger: The baseline with $4.2$x capacity is slightly worse than the baseline with $3.3$x capacity. We visualize the training curve in Figure~\ref{exp:capacity}: our method converges faster and has much better performance than large capacity baselines. 



\vspace{-0.07in}
\subsection{Comparison with Single Task Policy}
\vspace{-0.07in}


A substantial advantage of multi-task learning is with sample efficiency. We compare our policy with single task policy on MT10-Conditioned. Given 15 million samples, Ours (Shallow) achieve $71.8\%$ average success rate, while by average single task policy achieved $78.5\%$ success rate.
% (Note that independent trained SAC used 15 million samples per task)
Though the single task policy can overfit easily given enough training examples and achieve a very good result for one specific task but our method can still perform reasonably close to the single task policy, even we train with much fewer examples with much fewer parameters via a shared network. It shows that for each task, using data from other tasks along with our method can significantly improve sample efficiency, and skills learned by the soft modular policy can be shared between tasks with routing.

\vspace{-0.07in}
\subsection{Analysing Learning Components}
\vspace{-0.07in}
We analyze the importance of two learning components in our method with MT10-Conditioned: (i) Balance the training across different tasks using temperature parameters (Eq.~\ref{eq:ecsac:multitask_alpha_objective}); (ii) Use observation representation as the inputs for the routing network. 
We report the comparison results in Figure~\ref{exp:ablation1} and Figure~\ref{exp:ablation2}. In Figure~\ref{exp:ablation2}, we ablate our method in the Ours (Shallow) setting, and remove the balance training (Ours (Shallow, w/o Balance)) as well as remove both the balance training and observation inputs for the routing network (Ours (Shallow, w/o Obs $\&$ Balance)).
% With our approach, we can reach around $70\%$ success rate across 10 tasks. 
If we remove one or both learning components, the success rate is reduced by a large margin. Thus both components play an important role in our approach.
The importance of encoding observation for our routing network might also be a reason for the poor performance of Hard Routing baseline, since the controller of Hard Routing is parameterized by tabular lookup table which can not encode high dimensional information like observation.
% We also add the balance training strategy in optimizing the baseline approach
We also apply the balance training strategy to the baseline
in Figure~\ref{exp:ablation2} as MT-MH-SAC-Balance. Interestingly, we find that the baseline approach is not affected as much with the new optimization strategy. Thus we do not apply balance training for baselines. 
